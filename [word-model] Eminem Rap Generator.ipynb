{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils.data_utils import get_file\n",
    "from random import randint\n",
    "from collections import OrderedDict \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import re\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = pd.read_csv('data/travis-scott-lyrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_lines(df):\n",
    "    words = []\n",
    "    \n",
    "    for index, row in df['lyrics'].iteritems():\n",
    "        row = str(row).lower()\n",
    "        for line in row.split('|-|'):\n",
    "            new_words = re.findall(r\"\\b[a-z']+\\b\", unidecode(line))\n",
    "            words = words + new_words\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lyric_lines = get_tokenized_lines(songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 15330\n"
     ]
    }
   ],
   "source": [
    "SEQ_LENGTH = 50 + 1\n",
    "sequences = list()\n",
    "\n",
    "for i in range(SEQ_LENGTH, len(all_lyric_lines)):\n",
    "    seq = all_lyric_lines[i - SEQ_LENGTH: i]\n",
    "    sequences.append(seq)\n",
    "\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_doc(lines, filename):\n",
    "    for line in lines:\n",
    "        data = ' '.join(line)\n",
    "        \n",
    "    '\\n'.join(data)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_filename = 'sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"'s\": 0, 'a': 1, 'aaaw': 2, 'about': 3, 'absolute': 4, 'accountant': 5, 'act': 6, 'actavis': 7, 'acting': 8, 'addiction': 9, 'address': 10, 'addressing': 11, 'adidas': 12, 'afford': 13, 'after': 14, 'again': 15, 'ages': 16, 'ago': 17, 'agree': 18, 'ahead': 19, 'ahh': 20, 'ahhh': 21, 'ahhhh': 22, \"ain't\": 23, 'akon': 24, 'album': 25, 'alcohol': 26, 'alert': 27, 'alive': 28, 'all': 29, 'almost': 30, 'alone': 31, 'already': 32, 'alright': 33, 'always': 34, 'am': 35, 'amazing': 36, 'amen': 37, 'amenities': 38, 'an': 39, 'and': 40, 'angel': 41, 'angels': 42, 'angles': 43, 'anna': 44, 'another': 45, 'answer': 46, 'anthem': 47, 'anti': 48, 'antidote': 49, 'any': 50, 'anything': 51, 'aod': 52, 'ap': 53, 'apparently': 54, 'appetite': 55, 'applebees': 56, 'are': 57, 'argue': 58, 'argument': 59, 'around': 60, 'artists': 61, 'as': 62, 'ask': 63, 'asked': 64, 'askin': 65, 'asleep': 66, 'ass': 67, 'astro': 68, 'astronomical': 69, 'astroworld': 70, 'at': 71, 'atlanta': 72, 'automatic': 73, 'avenue': 74, 'aw': 75, 'away': 76, 'ay': 77, 'aye': 78, 'ayy': 79, 'b': 80, \"b's\": 81, 'baby': 82, 'back': 83, 'backflip': 84, 'backup': 85, 'backwood': 86, 'backwoods': 87, 'bad': 88, 'badass': 89, 'baes': 90, 'bag': 91, 'baguettes': 92, 'balance': 93, 'bald': 94, 'ball': 95, 'bands': 96, 'bang': 97, 'bank': 98, 'bankroll': 99, 'bare': 100, 'barely': 101, 'barre': 102, 'bartender': 103, 'basement': 104, 'bathed': 105, 'bathhouse': 106, 'batmobile': 107, 'be': 108, 'beach': 109, 'beam': 110, 'beamin': 111, 'beans': 112, 'bear': 113, 'beast': 114, 'beastin': 115, 'beat': 116, 'because': 117, 'bedroom': 118, 'beef': 119, 'been': 120, 'bees': 121, 'before': 122, 'beginning': 123, 'behind': 124, 'bein': 125, 'believe': 126, 'believing': 127, 'belt': 128, 'bend': 129, 'bentley': 130, 'benz': 131, 'beside': 132, 'besides': 133, 'best': 134, 'bet': 135, 'better': 136, 'between': 137, 'bezel': 138, 'bezerk': 139, 'bezos': 140, 'biani': 141, 'biased': 142, 'bible': 143, 'bieber': 144, 'big': 145, 'bigger': 146, 'bih': 147, 'bike': 148, 'bills': 149, 'bird': 150, 'birds': 151, 'birthday': 152, 'bitch': 153, 'bitches': 154, 'bite': 155, 'bitin': 156, 'black': 157, 'blanco': 158, 'bleed': 159, 'bleedin': 160, 'bleeds': 161, 'blessed': 162, 'blessings': 163, 'block': 164, 'blocks': 165, 'blood': 166, 'bloody': 167, 'blow': 168, 'blowin': 169, 'blowing': 170, 'blue': 171, 'blues': 172, 'bodies': 173, 'body': 174, 'bone': 175, 'bonnaroo': 176, 'boo': 177, \"boo'd\": 178, 'booch': 179, 'booger': 180, 'book': 181, 'boomin': 182, 'booth': 183, 'booze': 184, 'bored': 185, 'boss': 186, 'both': 187, 'bother': 188, 'bottle': 189, 'bottles': 190, 'bottom': 191, 'bought': 192, 'bounce': 193, 'bourse': 194, 'bout': 195, 'bowl': 196, 'boxes': 197, 'boy': 198, 'boys': 199, 'bracelets': 200, 'braids': 201, 'brain': 202, 'brands': 203, 'brazy': 204, 'bread': 205, 'break': 206, 'breakfast': 207, 'breakin': 208, 'breathe': 209, 'breeze': 210, 'brian': 211, 'bricks': 212, 'brighter': 213, 'brightest': 214, 'bring': 215, 'bro': 216, 'broad': 217, 'broads': 218, 'bronco': 219, 'bros': 220, 'brothel': 221, 'brother': 222, 'brought': 223, 'brr': 224, 'brrrapt': 225, 'brrrrr': 226, 'brrrrrr': 227, 'brunch': 228, 'buddy': 229, 'building': 230, 'built': 231, 'bull': 232, 'bulletproof': 233, 'bullets': 234, 'bun': 235, 'bungie': 236, 'bunkin': 237, 'bunky': 238, 'burn': 239, 'burton': 240, 'bust': 241, 'bustin': 242, 'but': 243, 'buy': 244, 'by': 245, 'c': 246, 'caesar': 247, 'cage': 248, 'cake': 249, 'call': 250, 'called': 251, 'callin': 252, 'calling': 253, 'calls': 254, 'calm': 255, 'came': 256, 'camel': 257, 'camera': 258, 'cameraman': 259, 'can': 260, \"can't\": 261, 'canada': 262, 'cancel': 263, 'candy': 264, 'cane': 265, 'cannot': 266, 'captain': 267, 'car': 268, 'carats': 269, 'care': 270, 'careful': 271, 'carnivals': 272, 'case': 273, 'cash': 274, 'castle': 275, 'catch': 276, 'caught': 277, 'cause': 278, 'celebrate': 279, 'celibate': 280, 'celine': 281, 'celly': 282, 'certain': 283, 'chain': 284, 'chains': 285, 'champ': 286, 'change': 287, 'charges': 288, 'chase': 289, 'cheat': 290, 'check': 291, 'checked': 292, 'checkin': 293, 'checks': 294, 'cheffin': 295, 'cheque': 296, 'chick': 297, 'chill': 298, 'chillin': 299, 'chills': 300, 'chips': 301, 'chopper': 302, 'choppers': 303, 'christ': 304, 'circle': 305, 'circles': 306, 'cirque': 307, 'city': 308, 'class': 309, 'claustrophobic': 310, 'clean': 311, 'clip': 312, 'clique': 313, 'close': 314, 'closed': 315, 'clothes': 316, 'clouds': 317, 'cloudy': 318, 'club': 319, 'clue': 320, 'coach': 321, 'cobain': 322, 'coca': 323, 'cocaina': 324, 'cochran': 325, 'coco': 326, 'cocoa': 327, 'code': 328, 'codeine': 329, 'cold': 330, 'coldplay': 331, 'colored': 332, 'come': 333, 'coming': 334, 'comments': 335, 'commit': 336, 'compared': 337, 'complicated': 338, 'computed': 339, 'condo': 340, 'conscious': 341, 'control': 342, 'controversial': 343, 'conversation': 344, 'cookin': 345, 'cool': 346, 'coolin': 347, 'cooper': 348, 'coordinate': 349, 'cop': 350, 'copped': 351, 'copy': 352, 'cougars': 353, 'could': 354, \"could've\": 355, \"couldn't\": 356, 'count': 357, 'counting': 358, 'coupe': 359, 'coupes': 360, 'couple': 361, 'cover': 362, 'cracked': 363, 'crackers': 364, 'crash': 365, 'crazy': 366, 'cream': 367, 'create': 368, 'creek': 369, 'creep': 370, 'creepin': 371, 'creeping': 372, 'crept': 373, 'crew': 374, 'crib': 375, 'cried': 376, 'cross': 377, 'cruise': 378, 'crushed': 379, 'crushes': 380, 'cry': 381, 'culkin': 382, 'cum': 383, 'cup': 384, 'curb': 385, 'curfew': 386, 'curry': 387, 'cut': 388, 'cute': 389, 'd': 390, 'dab': 391, 'daddy': 392, 'damn': 393, 'dance': 394, 'dancin': 395, 'dangerous': 396, 'dark': 397, 'dash': 398, 'dat': 399, 'dated': 400, 'daughter': 401, 'david': 402, 'dawg': 403, 'dawgs': 404, 'day': 405, 'days': 406, 'de': 407, 'dead': 408, 'deal': 409, 'dealin': 410, 'dealing': 411, 'dear': 412, 'debating': 413, 'deck': 414, 'deen': 415, 'deep': 416, 'deepest': 417, 'defeat': 418, 'defeated': 419, 'dem': 420, 'demons': 421, 'depart': 422, 'designer': 423, 'desk': 424, 'diabolical': 425, 'diamond': 426, 'diamonds': 427, 'dice': 428, 'dick': 429, 'did': 430, 'die': 431, 'different': 432, 'dig': 433, 'digest': 434, 'dinero': 435, 'dinnertime': 436, 'dinosaurus': 437, 'dip': 438, 'dipped': 439, 'dirt': 440, 'dirty': 441, 'discriminize': 442, 'disguise': 443, 'dissin': 444, 'divin': 445, 'dizzy': 446, 'dj': 447, 'do': 448, 'dodged': 449, 'dodgin': 450, 'dog': 451, 'dogs': 452, 'doheny': 453, 'doing': 454, 'dollars': 455, 'dolo': 456, \"don't\": 457, 'done': 458, 'door': 459, 'doors': 460, 'dope': 461, 'double': 462, 'dough': 463, 'dousin': 464, 'down': 465, 'downs': 466, 'dozen': 467, 'drake': 468, 'drank': 469, 'draped': 470, 'drawers': 471, 'dream': 472, 'dreams': 473, 'drill': 474, 'drink': 475, 'drinkin': 476, 'drinking': 477, 'drip': 478, 'drive': 479, 'driven': 480, 'driver': 481, 'driving': 482, 'drop': 483, 'dropped': 484, 'drove': 485, 'drown': 486, 'drugs': 487, 'drummers': 488, 'drunk': 489, 'du': 490, 'duck': 491, 'duffles': 492, 'duller': 493, 'dumb': 494, 'dumped': 495, 'dust': 496, 'dyin': 497, 'e': 498, 'ear': 499, 'ease': 500, 'easy': 501, 'eat': 502, 'eatin': 503, 'eating': 504, 'edge': 505, 'effect': 506, 'eh': 507, 'ehh': 508, 'elevator': 509, 'ellen': 510, 'else': 511, 'em': 512, 'emotions': 513, 'empty': 514, 'end': 515, 'ends': 516, 'enemies': 517, 'enemy': 518, 'energy': 519, 'enjoy': 520, 'enormous': 521, 'epi': 522, 'eric': 523, 'escape': 524, 'eses': 525, 'evade': 526, 'even': 527, 'ever': 528, 'every': 529, 'everybody': 530, 'everyone': 531, 'everythin': 532, 'everything': 533, 'everytime': 534, 'everywhere': 535, 'ex': 536, 'excellin': 537, 'except': 538, 'exciting': 539, 'exercise': 540, 'exes': 541, 'express': 542, 'eye': 543, 'eyes': 544, 'face': 545, 'faces': 546, 'facetime': 547, 'fade': 548, 'faith': 549, 'fake': 550, 'fall': 551, 'fallin': 552, 'falling': 553, 'family': 554, 'fans': 555, 'fantasy': 556, 'farewell': 557, 'fast': 558, 'faster': 559, 'fault': 560, 'favors': 561, 'fear': 562, 'fed': 563, 'feds': 564, 'feel': 565, 'feelin': 566, 'feeling': 567, 'feelings': 568, 'feels': 569, 'feet': 570, 'fell': 571, 'femalin': 572, 'fetish': 573, 'few': 574, 'field': 575, 'fifa': 576, 'fifteen': 577, 'fifty': 578, 'fight': 579, 'fightin': 580, 'figure': 581, 'fiji': 582, 'fill': 583, 'finally': 584, 'find': 585, 'fine': 586, 'finessing': 587, 'fire': 588, 'first': 589, 'fishcale': 590, 'fit': 591, 'five': 592, 'fix': 593, 'flame': 594, 'flashes': 595, 'flee': 596, 'flew': 597, 'flex': 598, 'flexin': 599, 'flight': 600, 'fling': 601, 'flipped': 602, 'flippin': 603, 'flood': 604, 'flooded': 605, 'floor': 606, 'flowin': 607, 'flush': 608, 'fly': 609, 'flyin': 610, 'focus': 611, 'follow': 612, 'followin': 613, 'fool': 614, 'foot': 615, 'for': 616, 'forbes': 617, 'fore': 618, 'foreign': 619, 'forever': 620, 'forgot': 621, 'formal': 622, 'forty': 623, 'forward': 624, 'found': 625, 'four': 626, 'fr': 627, 'freaks': 628, 'free': 629, 'freeway': 630, 'freezin': 631, 'freezing': 632, 'freight': 633, 'fresh': 634, 'friend': 635, 'friends': 636, 'frightening': 637, 'fro': 638, 'frog': 639, 'from': 640, 'front': 641, 'frost': 642, 'fruit': 643, 'fruits': 644, 'fuck': 645, 'fucked': 646, 'fuckin': 647, 'fucking': 648, 'fucks': 649, 'full': 650, 'fumes': 651, 'fun': 652, 'function': 653, 'funds': 654, 'g': 655, \"g's\": 656, 'gains': 657, 'game': 658, 'gang': 659, 'gangland': 660, 'garden': 661, 'garlic': 662, 'gate': 663, 'gave': 664, 'gears': 665, 'geekin': 666, 'get': 667, 'gettin': 668, 'getting': 669, 'ghetto': 670, 'gimme': 671, 'ginnies': 672, 'girl': 673, 'girls': 674, 'give': 675, 'glo': 676, 'glue': 677, 'glutes': 678, 'go': 679, 'goalie': 680, 'god': 681, 'goddamn': 682, 'goin': 683, 'going': 684, 'gold': 685, 'gon': 686, 'gone': 687, 'gonna': 688, 'good': 689, 'goose': 690, 'goosebumps': 691, 'got': 692, 'gots': 693, 'gotta': 694, 'gotten': 695, 'grace': 696, 'grandma': 697, 'granny': 698, 'green': 699, 'grind': 700, 'gringo': 701, 'griselda': 702, 'group': 703, 'guap': 704, 'guards': 705, 'guess': 706, 'guide': 707, 'gun': 708, 'guns': 709, 'guy': 710, 'h': 711, 'ha': 712, 'habibis': 713, 'had': 714, 'half': 715, 'halftime': 716, 'hallway': 717, 'hand': 718, 'handle': 719, 'hang': 720, 'hangin': 721, 'happen': 722, 'hard': 723, 'harden': 724, 'harder': 725, 'hate': 726, 'haters': 727, 'hatin': 728, 'have': 729, 'havin': 730, 'he': 731, \"he's\": 732, 'head': 733, 'heap': 734, 'hear': 735, 'heard': 736, 'heart': 737, \"heart's\": 738, 'heat': 739, 'heatin': 740, 'heaven': 741, 'hectic': 742, 'heimlich': 743, 'hell': 744, 'hella': 745, 'help': 746, 'henny': 747, 'her': 748, 'here': 749, 'hesitate': 750, 'hey': 751, 'hidden': 752, 'hide': 753, 'hidin': 754, 'high': 755, \"high's\": 756, 'higher': 757, 'highway': 758, 'hil': 759, 'hill': 760, 'hills': 761, 'him': 762, 'hint': 763, 'his': 764, 'hit': 765, 'hittin': 766, 'hm': 767, 'hmm': 768, 'ho': 769, 'hobbies': 770, 'hobby': 771, 'hockey': 772, 'hoe': 773, 'hoes': 774, 'hold': 775, 'hollywood': 776, 'home': 777, 'homescreens': 778, 'homie': 779, 'homies': 780, 'honey': 781, 'honeymoon': 782, 'hood': 783, 'hooked': 784, 'hoops': 785, 'hop': 786, 'hope': 787, 'hoping': 788, 'hopped': 789, 'horny': 790, 'horry': 791, 'horse': 792, 'hotter': 793, 'hotties': 794, 'hour': 795, 'hours': 796, 'house': 797, 'how': 798, 'however': 799, 'howl': 800, 'howlin': 801, 'huh': 802, 'hula': 803, 'hundred': 804, 'hundreds': 805, 'hypnotic': 806, 'i': 807, \"i'd\": 808, \"i'll\": 809, \"i'm\": 810, \"i'ma\": 811, \"i'mma\": 812, \"i've\": 813, 'ice': 814, 'iced': 815, 'icy': 816, 'ideas': 817, 'identity': 818, 'if': 819, 'ignore': 820, 'ike': 821, 'illuminati': 822, 'image': 823, 'impatient': 824, 'improbable': 825, 'in': 826, 'incidentals': 827, 'incline': 828, 'incomplete': 829, 'indian': 830, 'indict': 831, 'injuries': 832, 'inside': 833, 'instrumentals': 834, 'intelli': 835, 'interjected': 836, 'intertube': 837, 'into': 838, 'intonic': 839, 'invented': 840, 'ironic': 841, 'is': 842, 'island': 843, 'israeli': 844, 'issue': 845, 'it': 846, \"it's\": 847, 'itchin': 848, 'iv': 849, 'ivory': 850, 'jack': 851, 'jacques': 852, 'jamba': 853, 'james': 854, 'jane': 855, 'janeiro': 856, 'jaw': 857, \"jaw's\": 858, 'jawn': 859, 'jay': 860, 'jealous': 861, 'jeans': 862, 'jenner': 863, 'jersey': 864, 'jesus': 865, 'jet': 866, 'jeweler': 867, 'jewelers': 868, 'jewelry': 869, 'job': 870, 'johnny': 871, 'join': 872, 'jokes': 873, 'joking': 874, 'jolly': 875, 'jordan': 876, 'journey': 877, 'juice': 878, 'jump': 879, 'jumpin': 880, 'jumping': 881, 'june': 882, 'just': 883, 'justify': 884, 'jxm': 885, 'keep': 886, 'keepin': 887, 'keeping': 888, 'keeps': 889, 'keith': 890, 'kettle': 891, 'key': 892, 'keys': 893, 'kick': 894, 'kickin': 895, 'kid': 896, 'kiddin': 897, 'kidding': 898, 'kidneys': 899, 'kids': 900, 'kill': 901, 'killas': 902, 'killer': 903, 'killin': 904, 'kilos': 905, 'kind': 906, 'kiss': 907, 'kites': 908, 'klondike': 909, 'knees': 910, 'know': 911, 'kobe': 912, 'kylie': 913, 'l': 914, 'la': 915, 'label': 916, 'laced': 917, 'lacking': 918, 'ladies': 919, 'lady': 920, 'laferrari': 921, 'laflame': 922, 'lalalala': 923, 'lambo': 924, 'lame': 925, 'lancer': 926, 'land': 927, 'landed': 928, 'lanes': 929, 'lantern': 930, 'larry': 931, 'last': 932, 'late': 933, 'lately': 934, 'later': 935, 'latest': 936, 'laughin': 937, 'launchin': 938, 'law': 939, 'laws': 940, 'lawyer': 941, 'lay': 942, 'laying': 943, 'lead': 944, 'league': 945, 'lean': 946, 'leanin': 947, 'learned': 948, 'leave': 949, 'left': 950, 'legal': 951, 'lens': 952, 'less': 953, 'let': 954, \"let's\": 955, 'level': 956, 'lick': 957, 'licky': 958, 'lid': 959, 'lie': 960, 'life': 961, 'lift': 962, 'light': 963, 'lightin': 964, 'lightning': 965, 'lights': 966, 'like': 967, 'lil': 968, 'lime': 969, 'limelight': 970, 'line': 971, 'lines': 972, 'link': 973, 'links': 974, 'lips': 975, 'liquor': 976, 'listen': 977, 'listenin': 978, 'liston': 979, 'lit': 980, 'liter': 981, 'literally': 982, 'little': 983, 'live': 984, 'livin': 985, 'lizzy': 986, 'load': 987, 'loaded': 988, 'lobbies': 989, 'lobby': 990, 'lobster': 991, 'lock': 992, 'lockdown': 993, 'locked': 994, 'logo': 995, 'lonely': 996, 'long': 997, 'look': 998, 'lookin': 999, 'looking': 1000, 'loops': 1001, 'loose': 1002, 'loot': 1003, 'lord': 1004, 'lose': 1005, 'loss': 1006, 'lost': 1007, 'lot': 1008, 'lottery': 1009, 'loud': 1010, 'louis': 1011, 'love': 1012, 'lovin': 1013, 'loving': 1014, 'low': 1015, 'lowkey': 1016, 'lowrider': 1017, 'loyal': 1018, 'lucky': 1019, 'luh': 1020, 'luke': 1021, 'lunch': 1022, 'luther': 1023, 'm': 1024, \"m's\": 1025, 'ma': 1026, 'mac': 1027, 'macaulay': 1028, 'mad': 1029, 'made': 1030, 'magic': 1031, 'magnet': 1032, 'main': 1033, 'make': 1034, 'makin': 1035, 'mama': 1036, 'man': 1037, \"man's\": 1038, 'mantle': 1039, 'many': 1040, 'mariah': 1041, 'martin': 1042, 'matador': 1043, 'matter': 1044, 'mattress': 1045, 'max': 1046, 'may': 1047, 'maybe': 1048, \"mcdonald's\": 1049, 'mcknight': 1050, 'me': 1051, 'meadows': 1052, 'mean': 1053, 'meant': 1054, 'medals': 1055, 'medusa': 1056, 'meet': 1057, 'memory': 1058, 'mental': 1059, 'mentally': 1060, 'mercedes': 1061, 'mess': 1062, 'met': 1063, 'metro': 1064, 'mexico': 1065, 'michael': 1066, 'middle': 1067, 'might': 1068, 'mike': 1069, 'miles': 1070, 'milk': 1071, 'miller': 1072, 'million': 1073, 'mils': 1074, 'mind': 1075, 'mindful': 1076, 'mine': 1077, 'minefield': 1078, 'minute': 1079, 'misinformed': 1080, 'missiles': 1081, 'missing': 1082, 'mist': 1083, 'mistake': 1084, 'mistaken': 1085, 'mix': 1086, 'mixes': 1087, 'mixin': 1088, 'mo': 1089, 'mob': 1090, 'mobbin': 1091, 'moby': 1092, 'mode': 1093, 'models': 1094, 'molly': 1095, 'moment': 1096, 'moments': 1097, 'momma': 1098, 'monday': 1099, 'money': 1100, 'mood': 1101, 'moon': 1102, 'mop': 1103, 'morals': 1104, 'more': 1105, 'morning': 1106, 'mosh': 1107, 'most': 1108, 'mothafuckin': 1109, 'motherfucker': 1110, 'motherfuckin': 1111, 'motherfucking': 1112, 'motorists': 1113, 'mountains': 1114, 'mouth': 1115, 'move': 1116, 'moved': 1117, 'moves': 1118, 'movie': 1119, 'movin': 1120, 'moving': 1121, 'mr': 1122, 'much': 1123, 'mud': 1124, 'muddy': 1125, 'murda': 1126, 'murderin': 1127, 'murk': 1128, 'must': 1129, 'my': 1130, 'myself': 1131, 'n': 1132, 'nah': 1133, 'name': 1134, 'nappy': 1135, 'national': 1136, 'near': 1137, 'neck': 1138, 'need': 1139, 'needing': 1140, 'needs': 1141, 'nerves': 1142, 'neutered': 1143, 'never': 1144, 'new': 1145, 'news': 1146, 'newspaper': 1147, 'next': 1148, 'nice': 1149, 'nicole': 1150, 'niece': 1151, 'nigga': 1152, 'niggas': 1153, 'night': 1154, 'nights': 1155, 'nike': 1156, 'nine': 1157, 'no': 1158, 'nobody': 1159, 'nonchalant': 1160, 'none': 1161, 'noon': 1162, 'noose': 1163, 'nope': 1164, 'north': 1165, 'nose': 1166, 'not': 1167, 'nothin': 1168, 'nothing': 1169, 'now': 1170, 'nowhere': 1171, 'nude': 1172, 'nuder': 1173, 'numb': 1174, 'number': 1175, 'numbers': 1176, 'o': 1177, \"o'clock\": 1178, 'obama': 1179, 'occasions': 1180, 'ocean': 1181, \"od'd\": 1182, 'oeh': 1183, 'of': 1184, 'off': 1185, 'oh': 1186, 'ohhh': 1187, 'ohhhh': 1188, 'ohohoh': 1189, 'oj': 1190, 'ok': 1191, 'okay': 1192, 'old': 1193, 'ole': 1194, 'on': 1195, 'one': 1196, 'ones': 1197, 'only': 1198, 'onyx': 1199, 'ooh': 1200, 'oohh': 1201, 'ooo': 1202, 'oooh': 1203, 'ooooh': 1204, 'open': 1205, 'opioid': 1206, 'optimo': 1207, 'or': 1208, 'orbit': 1209, 'order': 1210, 'ordered': 1211, 'organs': 1212, 'ostritch': 1213, 'other': 1214, 'our': 1215, 'out': 1216, 'outchea': 1217, 'outside': 1218, 'outstandin': 1219, 'ovaries': 1220, 'over': 1221, 'overboard': 1222, 'overdrive': 1223, 'overheard': 1224, 'override': 1225, 'overstand': 1226, 'owe': 1227, 'p': 1228, 'pace': 1229, 'packed': 1230, 'packin': 1231, 'page': 1232, 'pageant': 1233, 'paid': 1234, 'pain': 1235, 'pants': 1236, 'panty': 1237, 'paper': 1238, 'papoose': 1239, 'parachute': 1240, 'parents': 1241, 'park': 1242, 'part': 1243, 'parts': 1244, 'party': 1245, 'pase': 1246, 'pass': 1247, 'passes': 1248, 'past': 1249, 'patch': 1250, 'patek': 1251, 'patient': 1252, 'pay': 1253, 'pde': 1254, 'peace': 1255, \"peace'd\": 1256, 'peaced': 1257, 'peat': 1258, 'pedestal': 1259, 'peek': 1260, 'peeling': 1261, 'people': 1262, 'pepper': 1263, 'perc': 1264, 'percocet': 1265, 'percs': 1266, 'perfect': 1267, 'perico': 1268, 'personal': 1269, 'personally': 1270, 'peter': 1271, 'phantom': 1272, 'pharmacist': 1273, \"pharmacy's\": 1274, 'phase': 1275, 'phelps': 1276, 'phil': 1277, 'philippe': 1278, 'phone': 1279, 'pick': 1280, 'picked': 1281, 'pickin': 1282, 'pics': 1283, 'picture': 1284, 'pictures': 1285, 'piece': 1286, 'piled': 1287, 'pillow': 1288, 'pills': 1289, 'pipe': 1290, 'piped': 1291, 'piper': 1292, 'pit': 1293, 'pizza': 1294, \"pj's\": 1295, 'place': 1296, 'plain': 1297, 'plan': 1298, 'plans': 1299, 'plant': 1300, 'plate': 1301, 'play': 1302, 'played': 1303, 'player': 1304, 'playin': 1305, 'playing': 1306, 'please': 1307, 'plenty': 1308, 'plug': 1309, 'plus': 1310, 'point': 1311, 'poison': 1312, 'polar': 1313, 'pole': 1314, 'poles': 1315, 'police': 1316, 'pool': 1317, 'pop': 1318, 'popped': 1319, 'poppin': 1320, 'popstar': 1321, 'porkchop': 1322, 'porridge': 1323, 'portland': 1324, 'pound': 1325, 'pour': 1326, 'poured': 1327, 'pouring': 1328, 'power': 1329, 'pre': 1330, 'preppin': 1331, 'press': 1332, 'pretend': 1333, 'prez': 1334, 'price': 1335, 'principal': 1336, 'probably': 1337, 'problem': 1338, 'problems': 1339, 'produced': 1340, 'projected': 1341, 'projects': 1342, 'promise': 1343, 'promised': 1344, 'propane': 1345, 'propellin': 1346, 'psychedelic': 1347, 'psychedelics': 1348, 'psychic': 1349, 'puck': 1350, 'pull': 1351, 'pulled': 1352, 'pullin': 1353, 'pump': 1354, 'purp': 1355, 'purple': 1356, 'push': 1357, 'pussy': 1358, 'put': 1359, 'puttin': 1360, 'putting': 1361, 'quavo': 1362, 'quest': 1363, 'quick': 1364, 'quit': 1365, 'quite': 1366, 'race': 1367, 'races': 1368, 'rack': 1369, 'rackin': 1370, 'racks': 1371, 'raf': 1372, 'rag': 1373, 'raging': 1374, 'rags': 1375, 'rain': 1376, 'ran': 1377, 'ranch': 1378, 'ranchers': 1379, 'range': 1380, 'ransom': 1381, 'rap': 1382, 'ratings': 1383, 'ratio': 1384, 'ray': 1385, 'ready': 1386, 'real': 1387, 'really': 1388, 'reason': 1389, 'rebuild': 1390, 'rebute': 1391, 'reckon': 1392, 'record': 1393, 'records': 1394, 'recruitin': 1395, 'red': 1396, 'reflex': 1397, 'reflexes': 1398, 'region': 1399, 'regret': 1400, 'relate': 1401, 'related': 1402, 'religion': 1403, 'reliving': 1404, 'relocate': 1405, 'remain': 1406, 'remedy': 1407, 'remind': 1408, 'reminds': 1409, 'remy': 1410, 'rental': 1411, 'rep': 1412, 'repellent': 1413, 'reset': 1414, 'residual': 1415, 'respect': 1416, 'responsible': 1417, 'rest': 1418, 'retreat': 1419, 'revelation': 1420, 'reverend': 1421, 'rex': 1422, 'rich': 1423, 'rick': 1424, 'ricky': 1425, 'rico': 1426, 'ride': 1427, 'rider': 1428, 'rides': 1429, 'ridin': 1430, 'riding': 1431, 'right': 1432, 'ring': 1433, 'rio': 1434, 'road': 1435, 'robbed': 1436, 'robert': 1437, 'rock': 1438, 'rockin': 1439, 'rocking': 1440, 'rockless': 1441, 'rockstar': 1442, 'rodeo': 1443, 'rolex': 1444, 'roll': 1445, 'rolled': 1446, 'rollie': 1447, 'rollies': 1448, 'rollin': 1449, 'rolling': 1450, 'rolls': 1451, 'roof': 1452, 'room': 1453, 'roommates': 1454, 'rope': 1455, 'rose': 1456, 'roster': 1457, 'rotation': 1458, 'round': 1459, 'ruined': 1460, 'ruler': 1461, 'rules': 1462, 'rumor': 1463, 'run': 1464, 'runnin': 1465, 'running': 1466, 's': 1467, 'sabertooth': 1468, 'sad': 1469, 'safe': 1470, 'said': 1471, 'saint': 1472, 'sak': 1473, 'same': 1474, 'sand': 1475, 'sanitize': 1476, 'santana': 1477, 'satan': 1478, 'satisfy': 1479, 'sauce': 1480, 'saucing': 1481, 'savage': 1482, 'savagery': 1483, 'save': 1484, 'saved': 1485, 'say': 1486, 'says': 1487, 'scandal': 1488, 'scars': 1489, 'scene': 1490, 'scholar': 1491, 'school': 1492, 'scoop': 1493, 'scoping': 1494, 'score': 1495, 'scores': 1496, 'scott': 1497, 'screen': 1498, 'screwed': 1499, 'screws': 1500, 'script': 1501, 'seal': 1502, 'seals': 1503, 'searchin': 1504, 'seat': 1505, 'seats': 1506, 'seattle': 1507, 'section': 1508, 'see': 1509, 'seed': 1510, 'seeing': 1511, 'seek': 1512, 'seem': 1513, 'seen': 1514, 'seh': 1515, 'seinfeld': 1516, 'sell': 1517, 'selling': 1518, 'semi': 1519, 'send': 1520, 'sendin': 1521, 'sending': 1522, 'sense': 1523, 'seriously': 1524, 'serve': 1525, 'service': 1526, 'servin': 1527, 'set': 1528, 'setting': 1529, 'settle': 1530, 'seven': 1531, 'shade': 1532, 'shady': 1533, 'shake': 1534, 'shaking': 1535, 'shalalala': 1536, 'share': 1537, 'shawty': 1538, 'she': 1539, \"she'll\": 1540, \"she's\": 1541, 'sheck': 1542, 'sherita': 1543, 'shhh': 1544, 'shine': 1545, 'shinin': 1546, 'shirt': 1547, 'shit': 1548, 'shoes': 1549, 'shook': 1550, 'shoot': 1551, 'shootin': 1552, 'shorties': 1553, 'shorty': 1554, 'shot': 1555, 'shotgunnin': 1556, 'shots': 1557, 'should': 1558, \"should've\": 1559, 'shout': 1560, 'show': 1561, 'showed': 1562, 'shows': 1563, 'showtime': 1564, 'shut': 1565, 'sicko': 1566, 'side': 1567, 'sideline': 1568, 'sight': 1569, 'sign': 1570, 'signs': 1571, 'silently': 1572, 'since': 1573, 'sing': 1574, 'single': 1575, \"sinner's\": 1576, 'sip': 1577, 'sippin': 1578, 'sipping': 1579, \"sister's\": 1580, 'sit': 1581, 'sitting': 1582, 'six': 1583, 'sixteen': 1584, 'sixty': 1585, 'skates': 1586, 'skin': 1587, 'skinnies': 1588, 'skip': 1589, 'skirt': 1590, 'skrr': 1591, 'skrrr': 1592, 'skrrt': 1593, 'skurt': 1594, 'sky': 1595, 'skywalkin': 1596, 'slaughter': 1597, 'sleep': 1598, 'sleepin': 1599, 'sleeping': 1600, 'sleepless': 1601, 'sleepy': 1602, 'slept': 1603, 'slidin': 1604, 'slop': 1605, 'slow': 1606, 'small': 1607, 'smash': 1608, 'smashin': 1609, 'smile': 1610, 'smoke': 1611, 'smokin': 1612, 'snapchat': 1613, 'snappin': 1614, 'sneak': 1615, 'sniffin': 1616, 'snipe': 1617, 'snooze': 1618, 'snorkelin': 1619, 'snortin': 1620, 'snorting': 1621, 'snow': 1622, 'so': 1623, 'soda': 1624, 'soft': 1625, 'sold': 1626, 'soleil': 1627, 'solo': 1628, 'some': 1629, 'someday': 1630, 'someone': 1631, 'somethin': 1632, 'sometimes': 1633, 'sonny': 1634, 'soon': 1635, 'sooner': 1636, 'sosa': 1637, 'soul': 1638, 'sound': 1639, 'south': 1640, 'southern': 1641, 'spare': 1642, 'speak': 1643, 'special': 1644, 'speed': 1645, 'spend': 1646, 'spending': 1647, 'spent': 1648, 'spicy': 1649, 'spill': 1650, 'spilled': 1651, 'spit': 1652, 'splish': 1653, 'split': 1654, 'splurge': 1655, 'spot': 1656, 'spotlight': 1657, 'sprinkled': 1658, 'sprinkler': 1659, 'squad': 1660, 'squid': 1661, 'sremm': 1662, 'stack': 1663, 'stackin': 1664, 'stacy': 1665, 'stage': 1666, 'stages': 1667, 'stand': 1668, 'star': 1669, 'stargazin': 1670, 'stars': 1671, 'start': 1672, 'started': 1673, 'stashin': 1674, 'state': 1675, 'static': 1676, 'status': 1677, 'stay': 1678, 'stayed': 1679, 'stayin': 1680, 'steady': 1681, 'steak': 1682, 'step': 1683, 'stick': 1684, 'still': 1685, 'sting': 1686, 'stinkin': 1687, 'stix': 1688, 'stoned': 1689, 'stop': 1690, 'store': 1691, 'straight': 1692, 'strange': 1693, 'street': 1694, 'streets': 1695, 'stressed': 1696, 'strike': 1697, 'strip': 1698, 'stripes': 1699, 'stripper': 1700, 'strippers': 1701, 'strong': 1702, 'stuck': 1703, 'stuff': 1704, 'stuntin': 1705, 'suck': 1706, 'suckin': 1707, 'sucking': 1708, 'sugar': 1709, 'suicide': 1710, 'suit': 1711, 'summer': 1712, 'sun': 1713, 'sunday': 1714, 'sunken': 1715, 'super': 1716, 'supposed': 1717, 'sure': 1718, 'surprised': 1719, 'survival': 1720, 'survive': 1721, 'sv': 1722, 'swang': 1723, 'swear': 1724, 'sweat': 1725, 'sweater': 1726, 'sweet': 1727, 'sweeter': 1728, 'swerve': 1729, 'swim': 1730, 'swimmin': 1731, 'swipe': 1732, 'switch': 1733, 'switchin': 1734, 'syrup': 1735, 't': 1736, 'take': 1737, 'takin': 1738, 'taking': 1739, 'talk': 1740, 'talkin': 1741, 'tamed': 1742, 'tan': 1743, 'tapping': 1744, 'taste': 1745, 'tattoos': 1746, 'tay': 1747, 'team': 1748, 'tec': 1749, 'tees': 1750, 'teeth': 1751, 'tell': 1752, 'tellin': 1753, 'ten': 1754, 'tennis': 1755, 'tester': 1756, 'texas': 1757, 'text': 1758, 'than': 1759, 'thanks': 1760, 'that': 1761, \"that's\": 1762, 'the': 1763, 'their': 1764, 'them': 1765, 'then': 1766, 'there': 1767, \"there's\": 1768, 'these': 1769, 'they': 1770, \"they'll\": 1771, \"they're\": 1772, 'thick': 1773, 'thing': 1774, 'things': 1775, 'think': 1776, 'thinkin': 1777, 'thinking': 1778, 'third': 1779, 'thirteen': 1780, 'this': 1781, 'those': 1782, 'though': 1783, 'thought': 1784, 'thoughts': 1785, 'thousand': 1786, 'thousands': 1787, 'threat': 1788, 'three': 1789, 'threw': 1790, 'throat': 1791, 'through': 1792, 'throw': 1793, 'throwin': 1794, 'throwing': 1795, 'thru': 1796, 'thugger': 1797, 'thumbin': 1798, 'tick': 1799, 'tidal': 1800, 'tie': 1801, 'ties': 1802, 'tiffany': 1803, 'tiger': 1804, 'tight': 1805, 'til': 1806, 'time': 1807, 'timeless': 1808, 'times': 1809, 'ting': 1810, 'tired': 1811, 'titties': 1812, 'to': 1813, 'today': 1814, 'toe': 1815, 'toes': 1816, 'together': 1817, 'toilet': 1818, 'told': 1819, 'toll': 1820, 'tom': 1821, 'tommy': 1822, 'tomorrow': 1823, 'tonight': 1824, 'too': 1825, 'took': 1826, 'top': 1827, 'torch': 1828, 'touch': 1829, 'touching': 1830, 'tour': 1831, 'tourist': 1832, 'towel': 1833, 'town': 1834, 'toyota': 1835, 'track': 1836, 'traffic': 1837, 'tragic': 1838, 'trap': 1839, 'trapped': 1840, 'traumatized': 1841, 'trav': 1842, 'travis': 1843, 'treason': 1844, 'treat': 1845, 'trees': 1846, 'tribe': 1847, 'tried': 1848, 'tries': 1849, 'trip': 1850, 'troops': 1851, 'trophy': 1852, 'tropics': 1853, 'truck': 1854, \"truck's\": 1855, 'true': 1856, 'trust': 1857, 'try': 1858, 'tryna': 1859, 'trynna': 1860, 'tuition': 1861, 'tune': 1862, 'turn': 1863, 'turned': 1864, 'turner': 1865, 'turnt': 1866, 'tv': 1867, 'tweak': 1868, 'tweakin': 1869, 'twelve': 1870, 'twice': 1871, 'two': 1872, 'type': 1873, 'uber': 1874, 'ufc': 1875, 'ugh': 1876, 'uh': 1877, 'uncle': 1878, 'under': 1879, 'understand': 1880, 'undertaker': 1881, 'unlawful': 1882, 'unless': 1883, 'up': 1884, 'ups': 1885, 'upset': 1886, 'us': 1887, 'use': 1888, 'used': 1889, 'v': 1890, 'vacant': 1891, 'valet': 1892, 'valium': 1893, 'vanilla': 1894, 'vatore': 1895, 'vegan': 1896, 'veins': 1897, 'venue': 1898, 'verge': 1899, 'versace': 1900, 'verse': 1901, 'vibe': 1902, 'vip': 1903, 'vision': 1904, 'visit': 1905, 'vivid': 1906, 'vocal': 1907, 'vodka': 1908, 'voice': 1909, 'vvs': 1910, \"vvs's\": 1911, 'wah': 1912, 'wait': 1913, 'waitin': 1914, 'waking': 1915, 'walk': 1916, 'walked': 1917, 'walkin': 1918, 'wan': 1919, 'wanna': 1920, 'want': 1921, 'wanted': 1922, 'war': 1923, 'ward': 1924, 'warzone': 1925, 'was': 1926, \"wasn't\": 1927, 'wasting': 1928, 'watch': 1929, 'watched': 1930, 'watchin': 1931, 'watching': 1932, 'water': 1933, 'waterfall': 1934, 'wave': 1935, 'waves': 1936, 'way': 1937, 'wayne': 1938, 'ways': 1939, 'waze': 1940, 'we': 1941, \"we'll\": 1942, \"we're\": 1943, 'weak': 1944, 'weakness': 1945, 'wearin': 1946, 'wee': 1947, 'weed': 1948, 'week': 1949, 'weekend': 1950, 'weekends': 1951, 'weight': 1952, 'well': 1953, 'went': 1954, 'were': 1955, 'west': 1956, 'wet': 1957, 'wetty': 1958, 'whacked': 1959, 'what': 1960, \"what's\": 1961, 'whatever': 1962, 'wheelie': 1963, 'when': 1964, 'whenever': 1965, 'where': 1966, \"where's\": 1967, 'wherever': 1968, 'whew': 1969, 'which': 1970, 'while': 1971, 'whip': 1972, 'whippin': 1973, 'white': 1974, 'who': 1975, 'whoa': 1976, \"whoever's\": 1977, 'whole': 1978, 'whoo': 1979, 'why': 1980, 'wide': 1981, 'wife': 1982, 'wifey': 1983, 'wig': 1984, 'wild': 1985, 'wildin': 1986, 'wildness': 1987, 'will': 1988, 'win': 1989, 'window': 1990, 'wings': 1991, 'wins': 1992, 'winter': 1993, \"winter's\": 1994, 'wippin': 1995, 'wise': 1996, 'wish': 1997, 'wit': 1998, 'with': 1999, 'without': 2000, 'woah': 2001, 'woman': 2002, \"won't\": 2003, 'wonderful': 2004, 'wonderin': 2005, 'woo': 2006, 'wooo': 2007, 'woooo': 2008, 'word': 2009, 'words': 2010, 'work': 2011, 'workin': 2012, 'working': 2013, 'works': 2014, 'world': 2015, 'worry': 2016, 'worst': 2017, 'would': 2018, 'wraith': 2019, 'wrappin': 2020, 'wreck': 2021, 'wrist': 2022, 'write': 2023, 'wrong': 2024, 'wu': 2025, 'x': 2026, 'xan': 2027, 'xanny': 2028, 'xans': 2029, \"y'all\": 2030, 'ya': 2031, \"ya'll\": 2032, 'yaaah': 2033, 'yaah': 2034, 'yah': 2035, 'yea': 2036, 'yeaaa': 2037, 'yeah': 2038, 'year': 2039, 'yee': 2040, 'yellin': 2041, 'yellow': 2042, 'yep': 2043, 'yet': 2044, 'you': 2045, \"you'll\": 2046, \"you're\": 2047, \"you've\": 2048, 'young': 2049, 'your': 2050, 'yours': 2051, 'zero': 2052, 'zone': 2053}\n",
      "vocabulary size: 2054\n"
     ]
    }
   ],
   "source": [
    "vocab = set(all_lyric_lines)\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "word_to_index = {w: i for i, w in enumerate(vocab)}\n",
    "index_to_word = {i: w for w, i in word_to_index.items()}\n",
    "word_indices = [word_to_index[word] for word in vocab]\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(word_to_index)\n",
    "print('vocabulary size: {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_lines(lines, seq_len):\n",
    "    tokenized = np.zeros((len(lines), seq_len))\n",
    "    \n",
    "    for r, line in enumerate(lines):\n",
    "        for c, word in enumerate(line):\n",
    "            tokenized[r, c] = word_to_index[word]\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_seq = get_tokenized_lines(sequences, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15330,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_seq[:, -1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "X, y = tokenized_seq[:, :-1], tokenized_seq[:, -1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape (15330, 50)\n",
      "y_shape (15330, 2054)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_shape\", X.shape)\n",
    "print(\"y_shape\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 50)            102700    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2054)              207454    \n",
      "=================================================================\n",
      "Total params: 461,054\n",
      "Trainable params: 461,054\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "120/120 [==============================] - 13s 110ms/step - loss: 6.4214 - accuracy: 0.0353\n",
      "Epoch 2/200\n",
      "120/120 [==============================] - 14s 116ms/step - loss: 6.0720 - accuracy: 0.0415\n",
      "Epoch 3/200\n",
      "120/120 [==============================] - 13s 111ms/step - loss: 5.9301 - accuracy: 0.0423\n",
      "Epoch 4/200\n",
      "120/120 [==============================] - 14s 114ms/step - loss: 5.8429 - accuracy: 0.0438\n",
      "Epoch 5/200\n",
      "120/120 [==============================] - 14s 115ms/step - loss: 5.7088 - accuracy: 0.0494\n",
      "Epoch 6/200\n",
      "120/120 [==============================] - 14s 117ms/step - loss: 5.5936 - accuracy: 0.0556\n",
      "Epoch 7/200\n",
      "120/120 [==============================] - 14s 117ms/step - loss: 5.4889 - accuracy: 0.0611\n",
      "Epoch 8/200\n",
      "120/120 [==============================] - 13s 107ms/step - loss: 5.3966 - accuracy: 0.0666\n",
      "Epoch 9/200\n",
      "120/120 [==============================] - 13s 105ms/step - loss: 5.3163 - accuracy: 0.0735\n",
      "Epoch 10/200\n",
      "120/120 [==============================] - 13s 107ms/step - loss: 5.2505 - accuracy: 0.0769\n",
      "Epoch 11/200\n",
      "120/120 [==============================] - 13s 106ms/step - loss: 5.1850 - accuracy: 0.0798\n",
      "Epoch 12/200\n",
      "120/120 [==============================] - 13s 106ms/step - loss: 5.1198 - accuracy: 0.0851\n",
      "Epoch 13/200\n",
      "120/120 [==============================] - 13s 107ms/step - loss: 5.0572 - accuracy: 0.0861\n",
      "Epoch 14/200\n",
      "120/120 [==============================] - 13s 109ms/step - loss: 4.9937 - accuracy: 0.0889\n",
      "Epoch 15/200\n",
      "120/120 [==============================] - 13s 109ms/step - loss: 4.9353 - accuracy: 0.0917\n",
      "Epoch 16/200\n",
      "120/120 [==============================] - 14s 116ms/step - loss: 4.8732 - accuracy: 0.0962\n",
      "Epoch 17/200\n",
      "120/120 [==============================] - 15s 122ms/step - loss: 4.8195 - accuracy: 0.1002\n",
      "Epoch 18/200\n",
      "120/120 [==============================] - 17s 141ms/step - loss: 4.7645 - accuracy: 0.1043\n",
      "Epoch 19/200\n",
      "120/120 [==============================] - 16s 135ms/step - loss: 4.7067 - accuracy: 0.1100\n",
      "Epoch 20/200\n",
      "120/120 [==============================] - 16s 131ms/step - loss: 4.6558 - accuracy: 0.1111\n",
      "Epoch 21/200\n",
      "120/120 [==============================] - 15s 122ms/step - loss: 4.5970 - accuracy: 0.1170\n",
      "Epoch 22/200\n",
      "120/120 [==============================] - 17s 142ms/step - loss: 4.5351 - accuracy: 0.1238\n",
      "Epoch 23/200\n",
      "120/120 [==============================] - 20s 167ms/step - loss: 4.4801 - accuracy: 0.1303\n",
      "Epoch 24/200\n",
      "120/120 [==============================] - 18s 151ms/step - loss: 4.4183 - accuracy: 0.1353\n",
      "Epoch 25/200\n",
      "120/120 [==============================] - 15s 124ms/step - loss: 4.3578 - accuracy: 0.1401\n",
      "Epoch 26/200\n",
      "120/120 [==============================] - 17s 141ms/step - loss: 4.2969 - accuracy: 0.1462\n",
      "Epoch 27/200\n",
      "120/120 [==============================] - 16s 136ms/step - loss: 4.2465 - accuracy: 0.1528\n",
      "Epoch 28/200\n",
      "120/120 [==============================] - 15s 128ms/step - loss: 4.1835 - accuracy: 0.1630\n",
      "Epoch 29/200\n",
      "120/120 [==============================] - 15s 129ms/step - loss: 4.1304 - accuracy: 0.1682\n",
      "Epoch 30/200\n",
      "120/120 [==============================] - 15s 126ms/step - loss: 4.0731 - accuracy: 0.1770\n",
      "Epoch 31/200\n",
      "120/120 [==============================] - 16s 131ms/step - loss: 4.0147 - accuracy: 0.1845\n",
      "Epoch 32/200\n",
      "120/120 [==============================] - 16s 135ms/step - loss: 3.9647 - accuracy: 0.1899\n",
      "Epoch 33/200\n",
      "120/120 [==============================] - 17s 140ms/step - loss: 3.9082 - accuracy: 0.1979\n",
      "Epoch 34/200\n",
      "120/120 [==============================] - 18s 147ms/step - loss: 3.8535 - accuracy: 0.2067\n",
      "Epoch 35/200\n",
      "120/120 [==============================] - 17s 138ms/step - loss: 3.8064 - accuracy: 0.2096\n",
      "Epoch 36/200\n",
      "120/120 [==============================] - 15s 126ms/step - loss: 3.7497 - accuracy: 0.2205\n",
      "Epoch 37/200\n",
      "120/120 [==============================] - 16s 134ms/step - loss: 3.6983 - accuracy: 0.2251\n",
      "Epoch 38/200\n",
      "120/120 [==============================] - 15s 128ms/step - loss: 3.6407 - accuracy: 0.2335\n",
      "Epoch 39/200\n",
      "120/120 [==============================] - 15s 129ms/step - loss: 3.5919 - accuracy: 0.2391\n",
      "Epoch 40/200\n",
      "120/120 [==============================] - 15s 121ms/step - loss: 3.5428 - accuracy: 0.2431\n",
      "Epoch 41/200\n",
      "120/120 [==============================] - 16s 134ms/step - loss: 3.4866 - accuracy: 0.2522\n",
      "Epoch 42/200\n",
      "120/120 [==============================] - 16s 130ms/step - loss: 3.4361 - accuracy: 0.2599\n",
      "Epoch 43/200\n",
      "120/120 [==============================] - 16s 130ms/step - loss: 3.3864 - accuracy: 0.2675\n",
      "Epoch 44/200\n",
      "120/120 [==============================] - 16s 134ms/step - loss: 3.3401 - accuracy: 0.2761\n",
      "Epoch 45/200\n",
      "120/120 [==============================] - 15s 128ms/step - loss: 3.2858 - accuracy: 0.2846\n",
      "Epoch 46/200\n",
      "120/120 [==============================] - 21s 173ms/step - loss: 3.2431 - accuracy: 0.2885\n",
      "Epoch 47/200\n",
      "120/120 [==============================] - 16s 135ms/step - loss: 3.1937 - accuracy: 0.2962\n",
      "Epoch 48/200\n",
      "120/120 [==============================] - 15s 126ms/step - loss: 3.1460 - accuracy: 0.3063\n",
      "Epoch 49/200\n",
      "120/120 [==============================] - 20s 165ms/step - loss: 3.1026 - accuracy: 0.3144\n",
      "Epoch 50/200\n",
      "120/120 [==============================] - 16s 133ms/step - loss: 3.0561 - accuracy: 0.3222\n",
      "Epoch 51/200\n",
      "120/120 [==============================] - 15s 121ms/step - loss: 3.0186 - accuracy: 0.3286\n",
      "Epoch 52/200\n",
      "120/120 [==============================] - 16s 131ms/step - loss: 2.9723 - accuracy: 0.3372\n",
      "Epoch 53/200\n",
      "120/120 [==============================] - 15s 122ms/step - loss: 2.9349 - accuracy: 0.3420\n",
      "Epoch 54/200\n",
      "120/120 [==============================] - 16s 136ms/step - loss: 2.8977 - accuracy: 0.3511\n",
      "Epoch 55/200\n",
      "120/120 [==============================] - 21s 172ms/step - loss: 2.8544 - accuracy: 0.3590\n",
      "Epoch 56/200\n",
      "120/120 [==============================] - 16s 130ms/step - loss: 2.8226 - accuracy: 0.3648\n",
      "Epoch 57/200\n",
      "120/120 [==============================] - 18s 146ms/step - loss: 2.7893 - accuracy: 0.3720\n",
      "Epoch 58/200\n",
      "120/120 [==============================] - 18s 154ms/step - loss: 2.7447 - accuracy: 0.3781\n",
      "Epoch 59/200\n",
      "120/120 [==============================] - 18s 153ms/step - loss: 2.7121 - accuracy: 0.3850\n",
      "Epoch 60/200\n",
      "120/120 [==============================] - 17s 140ms/step - loss: 2.6811 - accuracy: 0.3916\n",
      "Epoch 61/200\n",
      "120/120 [==============================] - 16s 131ms/step - loss: 2.6491 - accuracy: 0.3982\n",
      "Epoch 62/200\n",
      "120/120 [==============================] - 16s 134ms/step - loss: 2.6139 - accuracy: 0.4068\n",
      "Epoch 63/200\n",
      "120/120 [==============================] - 16s 132ms/step - loss: 2.5840 - accuracy: 0.4122\n",
      "Epoch 64/200\n",
      "120/120 [==============================] - 16s 134ms/step - loss: 2.5470 - accuracy: 0.4189\n",
      "Epoch 65/200\n",
      "120/120 [==============================] - 16s 131ms/step - loss: 2.5208 - accuracy: 0.4228\n",
      "Epoch 66/200\n",
      "120/120 [==============================] - 16s 131ms/step - loss: 2.4969 - accuracy: 0.4295\n",
      "Epoch 67/200\n",
      "120/120 [==============================] - 16s 132ms/step - loss: 2.4703 - accuracy: 0.4329\n",
      "Epoch 68/200\n",
      "120/120 [==============================] - 16s 135ms/step - loss: 2.4363 - accuracy: 0.4414\n",
      "Epoch 69/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 15s 129ms/step - loss: 2.4160 - accuracy: 0.4427\n",
      "Epoch 70/200\n",
      "120/120 [==============================] - 16s 130ms/step - loss: 2.3875 - accuracy: 0.4494\n",
      "Epoch 71/200\n",
      "120/120 [==============================] - 15s 127ms/step - loss: 2.3571 - accuracy: 0.4578\n",
      "Epoch 72/200\n",
      "120/120 [==============================] - 17s 144ms/step - loss: 2.3299 - accuracy: 0.4624\n",
      "Epoch 73/200\n",
      "120/120 [==============================] - 16s 137ms/step - loss: 2.3052 - accuracy: 0.4684\n",
      "Epoch 74/200\n",
      "120/120 [==============================] - 16s 133ms/step - loss: 2.2770 - accuracy: 0.4744\n",
      "Epoch 75/200\n",
      "120/120 [==============================] - 16s 131ms/step - loss: 2.2550 - accuracy: 0.4789\n",
      "Epoch 76/200\n",
      "120/120 [==============================] - 17s 144ms/step - loss: 2.2372 - accuracy: 0.4798\n",
      "Epoch 77/200\n",
      "120/120 [==============================] - 16s 131ms/step - loss: 2.2107 - accuracy: 0.4876\n",
      "Epoch 78/200\n",
      "120/120 [==============================] - 20s 168ms/step - loss: 2.1882 - accuracy: 0.4913\n",
      "Epoch 79/200\n",
      "120/120 [==============================] - 16s 133ms/step - loss: 2.1609 - accuracy: 0.5024\n",
      "Epoch 80/200\n",
      "120/120 [==============================] - 16s 135ms/step - loss: 2.1420 - accuracy: 0.5022\n",
      "Epoch 81/200\n",
      "120/120 [==============================] - 17s 144ms/step - loss: 2.1196 - accuracy: 0.5075\n",
      "Epoch 82/200\n",
      "120/120 [==============================] - 20s 169ms/step - loss: 2.0997 - accuracy: 0.5139\n",
      "Epoch 83/200\n",
      "120/120 [==============================] - 16s 135ms/step - loss: 2.0801 - accuracy: 0.5151\n",
      "Epoch 84/200\n",
      "120/120 [==============================] - 15s 124ms/step - loss: 2.0599 - accuracy: 0.5200\n",
      "Epoch 85/200\n",
      "120/120 [==============================] - 16s 135ms/step - loss: 2.0400 - accuracy: 0.5241\n",
      "Epoch 86/200\n",
      "120/120 [==============================] - 16s 131ms/step - loss: 2.0151 - accuracy: 0.5316\n",
      "Epoch 87/200\n",
      "120/120 [==============================] - 16s 131ms/step - loss: 1.9955 - accuracy: 0.5393\n",
      "Epoch 88/200\n",
      "120/120 [==============================] - 15s 123ms/step - loss: 1.9827 - accuracy: 0.5367\n",
      "Epoch 89/200\n",
      "120/120 [==============================] - 15s 127ms/step - loss: 1.9544 - accuracy: 0.5421\n",
      "Epoch 90/200\n",
      "120/120 [==============================] - 15s 129ms/step - loss: 1.9356 - accuracy: 0.5485\n",
      "Epoch 91/200\n",
      "120/120 [==============================] - 15s 123ms/step - loss: 1.9171 - accuracy: 0.5511\n",
      "Epoch 92/200\n",
      "120/120 [==============================] - 15s 125ms/step - loss: 1.9020 - accuracy: 0.5586\n",
      "Epoch 93/200\n",
      "120/120 [==============================] - 15s 123ms/step - loss: 1.8903 - accuracy: 0.5549\n",
      "Epoch 94/200\n",
      "120/120 [==============================] - 14s 118ms/step - loss: 1.8688 - accuracy: 0.5613\n",
      "Epoch 95/200\n",
      "120/120 [==============================] - 13s 107ms/step - loss: 1.8468 - accuracy: 0.5678\n",
      "Epoch 96/200\n",
      "120/120 [==============================] - 13s 106ms/step - loss: 1.8319 - accuracy: 0.5725\n",
      "Epoch 97/200\n",
      "120/120 [==============================] - 14s 120ms/step - loss: 1.8100 - accuracy: 0.5751\n",
      "Epoch 98/200\n",
      "120/120 [==============================] - 14s 119ms/step - loss: 1.7959 - accuracy: 0.5757\n",
      "Epoch 99/200\n",
      "120/120 [==============================] - 17s 141ms/step - loss: 1.7760 - accuracy: 0.5820\n",
      "Epoch 100/200\n",
      "120/120 [==============================] - 15s 121ms/step - loss: 1.7680 - accuracy: 0.5845\n",
      "Epoch 101/200\n",
      "120/120 [==============================] - 14s 114ms/step - loss: 1.7382 - accuracy: 0.5933\n",
      "Epoch 102/200\n",
      "120/120 [==============================] - 19s 161ms/step - loss: 1.7261 - accuracy: 0.5920\n",
      "Epoch 103/200\n",
      "120/120 [==============================] - 20s 166ms/step - loss: 1.7100 - accuracy: 0.5993\n",
      "Epoch 104/200\n",
      "120/120 [==============================] - 16s 130ms/step - loss: 1.6924 - accuracy: 0.5989\n",
      "Epoch 105/200\n",
      "120/120 [==============================] - 16s 134ms/step - loss: 1.6734 - accuracy: 0.6038\n",
      "Epoch 106/200\n",
      "120/120 [==============================] - 14s 116ms/step - loss: 1.6614 - accuracy: 0.6097\n",
      "Epoch 107/200\n",
      "120/120 [==============================] - 14s 113ms/step - loss: 1.6434 - accuracy: 0.6155\n",
      "Epoch 108/200\n",
      "120/120 [==============================] - 17s 138ms/step - loss: 1.6383 - accuracy: 0.6135\n",
      "Epoch 109/200\n",
      "120/120 [==============================] - 17s 139ms/step - loss: 1.6190 - accuracy: 0.6155\n",
      "Epoch 110/200\n",
      "120/120 [==============================] - 14s 115ms/step - loss: 1.6055 - accuracy: 0.6208\n",
      "Epoch 111/200\n",
      "120/120 [==============================] - 14s 116ms/step - loss: 1.5846 - accuracy: 0.6252\n",
      "Epoch 112/200\n",
      "120/120 [==============================] - 14s 114ms/step - loss: 1.5741 - accuracy: 0.6253\n",
      "Epoch 113/200\n",
      "120/120 [==============================] - 13s 110ms/step - loss: 1.5538 - accuracy: 0.6311\n",
      "Epoch 114/200\n",
      "120/120 [==============================] - 13s 111ms/step - loss: 1.5407 - accuracy: 0.6344\n",
      "Epoch 115/200\n",
      "120/120 [==============================] - 14s 115ms/step - loss: 1.5241 - accuracy: 0.6399\n",
      "Epoch 116/200\n",
      "120/120 [==============================] - 13s 112ms/step - loss: 1.5113 - accuracy: 0.6416\n",
      "Epoch 117/200\n",
      "120/120 [==============================] - 12s 102ms/step - loss: 1.4950 - accuracy: 0.6472\n",
      "Epoch 118/200\n",
      "120/120 [==============================] - 14s 118ms/step - loss: 1.4779 - accuracy: 0.6493\n",
      "Epoch 119/200\n",
      "120/120 [==============================] - 17s 138ms/step - loss: 1.4689 - accuracy: 0.6508\n",
      "Epoch 120/200\n",
      "120/120 [==============================] - 17s 138ms/step - loss: 1.4610 - accuracy: 0.6506\n",
      "Epoch 121/200\n",
      "120/120 [==============================] - 14s 121ms/step - loss: 1.4405 - accuracy: 0.6539\n",
      "Epoch 122/200\n",
      "120/120 [==============================] - 14s 120ms/step - loss: 1.4259 - accuracy: 0.6582\n",
      "Epoch 123/200\n",
      "120/120 [==============================] - 14s 117ms/step - loss: 1.4149 - accuracy: 0.6573\n",
      "Epoch 124/200\n",
      "120/120 [==============================] - 15s 124ms/step - loss: 1.3986 - accuracy: 0.6624\n",
      "Epoch 125/200\n",
      "120/120 [==============================] - 14s 116ms/step - loss: 1.3864 - accuracy: 0.6684\n",
      "Epoch 126/200\n",
      "120/120 [==============================] - 13s 110ms/step - loss: 1.3813 - accuracy: 0.6661\n",
      "Epoch 127/200\n",
      "120/120 [==============================] - 13s 107ms/step - loss: 1.3608 - accuracy: 0.6703\n",
      "Epoch 128/200\n",
      "120/120 [==============================] - 13s 108ms/step - loss: 1.3452 - accuracy: 0.6759\n",
      "Epoch 129/200\n",
      "120/120 [==============================] - 14s 113ms/step - loss: 1.3355 - accuracy: 0.6788\n",
      "Epoch 130/200\n",
      "120/120 [==============================] - 14s 113ms/step - loss: 1.3236 - accuracy: 0.6825\n",
      "Epoch 131/200\n",
      "120/120 [==============================] - 13s 109ms/step - loss: 1.3109 - accuracy: 0.6836\n",
      "Epoch 132/200\n",
      "120/120 [==============================] - 14s 119ms/step - loss: 1.2925 - accuracy: 0.6885\n",
      "Epoch 133/200\n",
      "120/120 [==============================] - 14s 120ms/step - loss: 1.2769 - accuracy: 0.6913\n",
      "Epoch 134/200\n",
      "120/120 [==============================] - 15s 123ms/step - loss: 1.2652 - accuracy: 0.6912\n",
      "Epoch 135/200\n",
      "120/120 [==============================] - 14s 119ms/step - loss: 1.2552 - accuracy: 0.6958\n",
      "Epoch 136/200\n",
      "120/120 [==============================] - 15s 123ms/step - loss: 1.2516 - accuracy: 0.6951\n",
      "Epoch 137/200\n",
      "120/120 [==============================] - 14s 116ms/step - loss: 1.2423 - accuracy: 0.6987\n",
      "Epoch 138/200\n",
      "120/120 [==============================] - 13s 105ms/step - loss: 1.2209 - accuracy: 0.7042\n",
      "Epoch 139/200\n",
      "120/120 [==============================] - 14s 117ms/step - loss: 1.2123 - accuracy: 0.7035\n",
      "Epoch 140/200\n",
      "120/120 [==============================] - 14s 120ms/step - loss: 1.1999 - accuracy: 0.7098\n",
      "Epoch 141/200\n",
      "120/120 [==============================] - 15s 122ms/step - loss: 1.1831 - accuracy: 0.7141\n",
      "Epoch 142/200\n",
      "120/120 [==============================] - 14s 119ms/step - loss: 1.1707 - accuracy: 0.7181\n",
      "Epoch 143/200\n",
      "120/120 [==============================] - 14s 120ms/step - loss: 1.1523 - accuracy: 0.7191\n",
      "Epoch 144/200\n",
      "120/120 [==============================] - 14s 120ms/step - loss: 1.1424 - accuracy: 0.7215\n",
      "Epoch 145/200\n",
      "120/120 [==============================] - 18s 146ms/step - loss: 1.1419 - accuracy: 0.7232\n",
      "Epoch 146/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 13s 106ms/step - loss: 1.1215 - accuracy: 0.7294\n",
      "Epoch 147/200\n",
      "120/120 [==============================] - 12s 104ms/step - loss: 1.1109 - accuracy: 0.7284\n",
      "Epoch 148/200\n",
      "120/120 [==============================] - 12s 102ms/step - loss: 1.1067 - accuracy: 0.7321\n",
      "Epoch 149/200\n",
      "120/120 [==============================] - 12s 103ms/step - loss: 1.0912 - accuracy: 0.7362\n",
      "Epoch 150/200\n",
      "120/120 [==============================] - 12s 103ms/step - loss: 1.0812 - accuracy: 0.7395\n",
      "Epoch 151/200\n",
      "120/120 [==============================] - 16s 134ms/step - loss: 1.0743 - accuracy: 0.7391\n",
      "Epoch 152/200\n",
      "120/120 [==============================] - 17s 143ms/step - loss: 1.0564 - accuracy: 0.7431\n",
      "Epoch 153/200\n",
      "120/120 [==============================] - 15s 126ms/step - loss: 1.0591 - accuracy: 0.7416\n",
      "Epoch 154/200\n",
      "120/120 [==============================] - 13s 111ms/step - loss: 1.0516 - accuracy: 0.7412\n",
      "Epoch 155/200\n",
      "120/120 [==============================] - 13s 111ms/step - loss: 1.0307 - accuracy: 0.7470\n",
      "Epoch 156/200\n",
      "120/120 [==============================] - 13s 108ms/step - loss: 1.0144 - accuracy: 0.7523\n",
      "Epoch 157/200\n",
      "120/120 [==============================] - 12s 102ms/step - loss: 1.0050 - accuracy: 0.7558\n",
      "Epoch 158/200\n",
      "120/120 [==============================] - 13s 105ms/step - loss: 0.9982 - accuracy: 0.7567\n",
      "Epoch 159/200\n",
      "120/120 [==============================] - 12s 99ms/step - loss: 0.9877 - accuracy: 0.7582\n",
      "Epoch 160/200\n",
      "120/120 [==============================] - 13s 108ms/step - loss: 0.9737 - accuracy: 0.7631\n",
      "Epoch 161/200\n",
      "120/120 [==============================] - 13s 109ms/step - loss: 0.9593 - accuracy: 0.7663\n",
      "Epoch 162/200\n",
      "120/120 [==============================] - 13s 105ms/step - loss: 0.9499 - accuracy: 0.7702\n",
      "Epoch 163/200\n",
      "120/120 [==============================] - 13s 105ms/step - loss: 0.9356 - accuracy: 0.7726\n",
      "Epoch 164/200\n",
      "120/120 [==============================] - 14s 115ms/step - loss: 0.9350 - accuracy: 0.7691\n",
      "Epoch 165/200\n",
      "120/120 [==============================] - 13s 110ms/step - loss: 0.9246 - accuracy: 0.7735\n",
      "Epoch 166/200\n",
      "120/120 [==============================] - 14s 117ms/step - loss: 0.9060 - accuracy: 0.7819\n",
      "Epoch 167/200\n",
      "120/120 [==============================] - 14s 114ms/step - loss: 0.8950 - accuracy: 0.7836\n",
      "Epoch 168/200\n",
      "120/120 [==============================] - 15s 127ms/step - loss: 0.8941 - accuracy: 0.7810\n",
      "Epoch 169/200\n",
      "120/120 [==============================] - 13s 112ms/step - loss: 0.8790 - accuracy: 0.7873\n",
      "Epoch 170/200\n",
      "120/120 [==============================] - 12s 99ms/step - loss: 0.8772 - accuracy: 0.7855\n",
      "Epoch 171/200\n",
      "120/120 [==============================] - 15s 129ms/step - loss: 0.8631 - accuracy: 0.7878\n",
      "Epoch 172/200\n",
      "120/120 [==============================] - 13s 110ms/step - loss: 0.8589 - accuracy: 0.7911\n",
      "Epoch 173/200\n",
      "120/120 [==============================] - 14s 117ms/step - loss: 0.8478 - accuracy: 0.7938\n",
      "Epoch 174/200\n",
      "120/120 [==============================] - 16s 136ms/step - loss: 0.8345 - accuracy: 0.7981\n",
      "Epoch 175/200\n",
      "120/120 [==============================] - 15s 124ms/step - loss: 0.8179 - accuracy: 0.7992\n",
      "Epoch 176/200\n",
      "120/120 [==============================] - 14s 117ms/step - loss: 0.8151 - accuracy: 0.8012\n",
      "Epoch 177/200\n",
      "120/120 [==============================] - 18s 148ms/step - loss: 0.7973 - accuracy: 0.8056\n",
      "Epoch 178/200\n",
      "120/120 [==============================] - 17s 141ms/step - loss: 0.7992 - accuracy: 0.8070\n",
      "Epoch 179/200\n",
      "120/120 [==============================] - 14s 116ms/step - loss: 0.7884 - accuracy: 0.8089\n",
      "Epoch 180/200\n",
      "120/120 [==============================] - 16s 130ms/step - loss: 0.7712 - accuracy: 0.8129\n",
      "Epoch 181/200\n",
      "120/120 [==============================] - 15s 126ms/step - loss: 0.7573 - accuracy: 0.8176\n",
      "Epoch 182/200\n",
      "120/120 [==============================] - 15s 129ms/step - loss: 0.7608 - accuracy: 0.8152\n",
      "Epoch 183/200\n",
      "120/120 [==============================] - 17s 139ms/step - loss: 0.7518 - accuracy: 0.8148\n",
      "Epoch 184/200\n",
      "120/120 [==============================] - 16s 130ms/step - loss: 0.7507 - accuracy: 0.8149\n",
      "Epoch 185/200\n",
      "120/120 [==============================] - 17s 139ms/step - loss: 0.7288 - accuracy: 0.8228\n",
      "Epoch 186/200\n",
      "120/120 [==============================] - 16s 130ms/step - loss: 0.7165 - accuracy: 0.8275\n",
      "Epoch 187/200\n",
      "120/120 [==============================] - 16s 131ms/step - loss: 0.7060 - accuracy: 0.8309\n",
      "Epoch 188/200\n",
      "120/120 [==============================] - 14s 114ms/step - loss: 0.7023 - accuracy: 0.8297\n",
      "Epoch 189/200\n",
      " 70/120 [================>.............] - ETA: 5s - loss: 0.6854 - accuracy: 0.8367"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X, y, batch_size=128, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.save('word_model_eminem.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('word_model_travis_scott.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_sequences(texts, word_to_index):\n",
    "    indices = np.zeros((1, len(texts)), dtype=int)\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        if text not in word_to_index:\n",
    "            random = index_to_word[randint(0,vocab_size)]\n",
    "            indices[:, i] = word_to_index[random]\n",
    "        else:\n",
    "            indices[:, i] = word_to_index[text]\n",
    "        \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pad_sequences(seq, maxlen):\n",
    "    start = seq.shape[1] - maxlen\n",
    "    \n",
    "    return seq[:, start: start + maxlen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model, word_to_index, seq_length, n_words):\n",
    "    generated = ''\n",
    "    usr_input = input(\"Write the beginning of your poem, the Drake machine will complete it. Your input is: \")\n",
    "    generated += usr_input \n",
    "    sys.stdout.write(\"\\n\\nHere is your poem: \\n\\n\") \n",
    "    \n",
    "    result = list()\n",
    "    in_text = [None] * 51\n",
    "    generated_list = generated.split()\n",
    "    \n",
    "    # if input is shorter than 51 words, fill the beginning with random words\n",
    "    if(len(generated_list) < 51):\n",
    "        end = len(generated_list)\n",
    "        for i in range (51 - end):\n",
    "            random = index_to_word[randint(0,vocab_size)]\n",
    "            in_text[i] = random\n",
    "            \n",
    "        index = 0\n",
    "        for i in range (51 - end, 51):\n",
    "            in_text[i] = generated_list[index]\n",
    "            index += 1\n",
    "\n",
    "    # if input is longer than 51 words, only use the last 51 words\n",
    "    if(len(generated_list) > 51):\n",
    "        end = len(generated_list)\n",
    "        in_text = generated_list[end-51:]\n",
    "\n",
    "    # generate words based on last 50 words\n",
    "    for _ in range(n_words):\n",
    "        encoded = texts_to_sequences(in_text[1:], word_to_index)\n",
    "        encoded = my_pad_sequences(encoded, maxlen=seq_length)\n",
    "        \n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        out_word = ''\n",
    "    \n",
    "        for word, index in word_to_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        \n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "        \n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the beginning of your poem, the Drake machine will complete it. Your input is: sdahfjashdfkjashd asdfh\n",
      "\n",
      "\n",
      "Here is your poem: \n",
      "\n",
      "['coke', 'cookin', \"problem's\", 'headache', 'fifth', 'ovulating', 'birth', 'dear', 'neither', 'grips', 'aight', 'pacquiao', 'truthful', 'mann', 'babe', 'misconducts', 'extraterrestrial', 'dodgers', 'smack', 'ruin', 'jade', 'chasing', 'cars', 'poppins', 'tear', 'legendary', 'sorry', 'plenty', 'lewinsky', 'outdo', 'raps', 'growing', 'wide', 'sister', \"you'ont\", 'opposites', 'winks', 'glide', 'huge', 'southpaw', 'grasp', 'symbolic', 'harris', 'come', 'helped', 'muster', 'sit', 'beer', 'sweden', 'sdahfjashdfkjashd', 'asdfh']\n",
      "envision fellatio fellow fellatio fellow denver clothes flippity christmas flippity cat triangle uh sa minimum fellatio fellow fellatio fellow fellatio groups resentment lazy hangs pectations jump pigeonholed ho's pinkett clothes pinkett clothes pinkett clothes pinkett lazy hangs bury idaho west pigeonholed fellatio fellow fellatio fellow fellatio fellow denver clothes pinkett\n"
     ]
    }
   ],
   "source": [
    "generated = generate_seq(model, word_to_index, seq_length, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
